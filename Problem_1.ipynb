{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Glorious Imports for data manipulation\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Our Glorious Imports for Models \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Robi Datathon 3.0 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_df = pd.read_csv(\"./dataset/purchase.csv\") # Train SET\n",
    "problem_df = pd.read_csv(\"./dataset/problem 1.csv\") # Test SET\n",
    "\n",
    "# get the total unique MAGIC_KEYS in file\n",
    "train_keys = np.unique(purchase_df[\"MAGIC_KEY\"].to_list())\n",
    "test_keys = np.unique(problem_df[\"MAGIC_KEY\"].to_list())\n",
    "\n",
    "# Debugging\n",
    "print(f'Unique keys in train set: {len(train_keys)}')\n",
    "print(f'Unique keys in test set: {len(test_keys)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Magic Key is in out test set\n",
    "temp_df = purchase_df[purchase_df['MAGIC_KEY'].isin(test_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning \n",
    "purchase_df = purchase_df[purchase_df['BOX_COUNT'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse  datetime column and set it as index\n",
    "def parse_datetime(df):\n",
    "    df['PURCHASE_DATE'] = pd.to_datetime(df['PURCHASE_DATE'], format=\"%d/%m/%Y\")\n",
    "    df.set_index('PURCHASE_DATE', inplace=True)\n",
    "    return df\n",
    "\n",
    "# more housekeeping\n",
    "temp_df = parse_datetime(temp_df)\n",
    "purchase_df = parse_datetime(purchase_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Groups of timescale dataset\n",
    "def slice_dataframe_by_dates(df):\n",
    "    days_groups = []\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2018-10-01' and PURCHASE_DATE <= '2018-10-15'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2018-10-16' and PURCHASE_DATE <= '2018-10-31'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2018-11-01' and PURCHASE_DATE <= '2018-11-15'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2018-11-16' and PURCHASE_DATE <= '2018-11-30'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2018-12-01' and PURCHASE_DATE <= '2018-12-15'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2018-12-16' and PURCHASE_DATE <= '2018-12-31'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2019-01-01' and PURCHASE_DATE <= '2019-01-15'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2019-01-16' and PURCHASE_DATE <= '2019-01-31'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2019-02-01' and PURCHASE_DATE <= '2019-02-15'\") )\n",
    "    days_groups.append( df.query(\"PURCHASE_DATE >= '2019-02-16' and PURCHASE_DATE <= '2019-02-28'\") )\n",
    "    return days_groups\n",
    "\n",
    "# get smaller slices of my groups\n",
    "test_purchase_groups = slice_dataframe_by_dates(temp_df)\n",
    "all_purchase_groups = slice_dataframe_by_dates(purchase_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the small slices make the main set\n",
    "def create_dataset_from_groups(unique_keys_list, days_groups):\n",
    "    new_dataset = pd.DataFrame()\n",
    "    new_dataset['MAGIC_KEY'] = unique_keys_list\n",
    "\n",
    "    for days_groupname, days_group_data in enumerate(days_groups):\n",
    "        # print(\"Group:\", days_groupname)\n",
    "        new_dataset[f'col_{days_groupname}'] = new_dataset['MAGIC_KEY'].isin(days_group_data['MAGIC_KEY']).astype(np.uint8)\n",
    "    return new_dataset\n",
    "\n",
    "# Assuming test_purchase_groups, temp_df, test_keys, and all_purchase_groups are defined\n",
    "days_groups = test_purchase_groups\n",
    "dataset = temp_df\n",
    "\n",
    "# Create test and train datasets\n",
    "\n",
    "test_df = create_dataset_from_groups(test_keys, test_purchase_groups)\n",
    "train_df = create_dataset_from_groups(train_keys, all_purchase_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming For conviniyence \n",
    "def process_dataset(df):\n",
    "    col_name_maps = {\n",
    "        'col_0': '1st',\n",
    "        'col_1': '2nd',\n",
    "        'col_2': '3rd',\n",
    "        'col_3': '4th',\n",
    "        'col_4': '5th',\n",
    "        'col_5': '6th',\n",
    "        'col_6': '7th',\n",
    "        'col_7': '8th',\n",
    "        'col_8': '9th',\n",
    "        'col_9': '10th',}\n",
    "    df = df.rename(col_name_maps, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# make train-test set\n",
    "test_df = process_dataset(test_df)\n",
    "train_df = process_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset\n",
    "test_df.to_csv('data/problem-1-test.csv', index=False)\n",
    "train_df.to_csv('data/problem-1-train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Preprocessed dataset and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "problem_df = pd.read_csv('./dataset/problem 1.csv')\n",
    "train_dataset = pd.read_csv('./preprocessed_dataset/preprocessed_train_dataset.csv')\n",
    "test_dataset = pd.read_csv('./preprocessed_dataset/preprocessed_test_dataset.csv')\n",
    "\n",
    "train_dataset.shape, test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'MAGIC_KEY'\n",
    "train_dataset.drop('MAGIC_KEY', axis=1, inplace=True)\n",
    "magic_keys = list(test_dataset['MAGIC_KEY'])\n",
    "test_dataset.drop('MAGIC_KEY', axis=1, inplace=True)\n",
    "\n",
    "train_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmap(df):\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "    # Create a heatmap plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True)  # Annotate cells with correlation values\n",
    "    plt.title('Correlation Heatmap')  # Set the title for the plot\n",
    "    plt.show()  # Display the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show heatmap for better understanding\n",
    "show_heatmap(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting and Formatting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test format\n",
    "def get_train_test(df, test_size = None):\n",
    "    Y = df['10th']\n",
    "    X = df.drop('10th', axis=1,)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    return (x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily split the train set for understanding model perfoemance\n",
    "x_train, x_test, y_train, y_test = get_train_test(train_dataset.sample(20000), 0.2)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models with parameters for checking the best fit model\n",
    "\n",
    "model_params = {\n",
    "    'svm':{\n",
    "        'model': SVC(gamma='auto'),\n",
    "        'params': {\n",
    "            'C': [1,10,20, 30, 50, 100, 500],\n",
    "            'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "        }\n",
    "    },\n",
    "    'random_forest':{\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [1, 5, 10, 18, 20, 30],\n",
    "            'n_jobs': [-1],\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression':{\n",
    "        'model': LogisticRegression(solver='liblinear', multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1,5,10],\n",
    "            'n_jobs': [-1],\n",
    "            'max_iter': [1000],\n",
    "        }\n",
    "    },\n",
    "    'decision_tree':{\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        }\n",
    "    },\n",
    "    'multinomial_nb':{\n",
    "        'model': MultinomialNB(), \n",
    "        'params': {\n",
    "            \n",
    "        }\n",
    "    },\n",
    "    'gaussian_nb':{\n",
    "        'model': GaussianNB(), \n",
    "        'params': {\n",
    "            \n",
    "        }\n",
    "    },\n",
    "    'gradient-boosting':{\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'loss': ['log_loss', 'exponential'],\n",
    "            'learning_rate': [0.1, 0.01, 0.001]\n",
    "        }\n",
    "    },\n",
    "    'hist-gradient-boosting':{\n",
    "        'model': HistGradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'max_iter': [1000, 500]\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model scores\n",
    "scores = []\n",
    "\n",
    "# Find out Our best fitting Model \n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
    "    clf.fit(x_train, y_train)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best results of the models\n",
    "df_res = pd.DataFrame(scores,columns=['model', 'best_score', 'best_params'])\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Using the Best Model and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual checking model performence \n",
    "def fit_and_evaluate(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    # Get scores on the test data\n",
    "    result = model.score(x_test, y_test)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling algorithm for best match\n",
    "def rolling_algorithm(df):\n",
    "    df['1st'] = df['2nd']\n",
    "    df['2nd'] = df['3rd']\n",
    "    df['3rd'] = df['4th']\n",
    "    df['4th'] = df['5th']\n",
    "    df['5th'] = df['6th']\n",
    "    df['6th'] = df['7th']\n",
    "    df['7th'] = df['8th']\n",
    "    df['8th'] = df['9th']\n",
    "    df['9th'] = df['10th']\n",
    "    df = df.drop('10th', axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predicted result in a CSV\n",
    "def save_results(magic_keys, predicts, file_name):\n",
    "    # Create a dictionary to map predictions (0 or 1) to corresponding labels (\"N\" or \"Y\")\n",
    "    replacements = {0: \"N\", 1: \"Y\"}\n",
    "    \n",
    "    # Convert the numeric predictions to labels using the dictionary\n",
    "    data = [replacements.get(x) for x in predicts]\n",
    "\n",
    "    # Create a DataFrame with MAGIC_KEY and PURCHASE columns\n",
    "    final_df = pd.DataFrame({\n",
    "        'MAGIC_KEY': magic_keys,\n",
    "        'PURCHASE': data\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    final_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model, predict on the test set, and store results\n",
    "def fit_predict_and_save(model, x_train, y_train, test_ds, X_test, Y_test, magic_keys, file_name):\n",
    "    # Fit the model on the training data\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict on the test dataset\n",
    "    predicts = model.predict(test_ds)\n",
    "    \n",
    "    # Calculate predictions on the X_test data\n",
    "    y_hats = model.predict(X_test)\n",
    "    \n",
    "    # Print the test score (accuracy)\n",
    "    print(f'Test Score: {accuracy_score(Y_test, y_hats)}')\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    save_results(magic_keys, predicts, file_name)\n",
    "    \n",
    "    # Print a success message\n",
    "    print(f'File: {file_name} Stored Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset for final training  \n",
    "def get_final_train_data(df):\n",
    "    Y = df['9th']\n",
    "    X = df.drop(['10th', '9th'], axis=1,)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_dataset.iloc[:, :8].to_numpy()\n",
    "test_Y = test_dataset.iloc[:, 8].to_numpy()\n",
    "\n",
    "test_dataset1 = test_dataset.iloc[:, 2:10].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual train test splitting for final training and test data\n",
    "X, Y = get_final_train_data(train_dataset)\n",
    "# test_dataset = rolling_algorithm(test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "X.shape, test_dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on final model\n",
    "model = RandomForestClassifier(n_estimators=18, max_depth=15, n_jobs=-1, verbose=1)\n",
    "fit_predict_and_save(model, X, Y, test_dataset1, test_X, test_Y, magic_keys, \"./solution/submission_random_forest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.15",
   "language": "python",
   "name": "tf2.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
